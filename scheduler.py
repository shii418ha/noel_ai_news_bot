from rss_collector import fetch_all_articles
from summarizer import summarize_article
from discord_poster import post_to_discord
from posted_tracker import load_posted_urls, save_posted_url
import json
from datetime import datetime, timedelta
import dateutil.parser

# è¨˜äº‹ã®å…¬é–‹æ—¥ãŒæœ€è¿‘ã‹ã©ã†ã‹ã‚’åˆ¤å®š
def is_recent(published_str, threshold_minutes=1440):
    try:
        published_dt = dateutil.parser.parse(published_str)
        return (datetime.utcnow() - published_dt).total_seconds() < threshold_minutes * 60
    except Exception:
        return False

def run():
    raw_articles = fetch_all_articles()
    print(f"ğŸ’¡ å–å¾—è¨˜äº‹æ•°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‰ï¼‰: {len(raw_articles)}")

    # ğŸ‘‡ è¨˜äº‹ã®ä¸­èº«ã‚’å…¨éƒ¨å‡ºåŠ›
    for a in raw_articles:
        print(json.dumps(a, indent=2, ensure_ascii=False))

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å‡¦ç†
    articles = sorted(raw_articles, key=lambda x: x.get("published", ""), reverse=True)
    articles = [a for a in articles if is_recent(a.get("published", ""), 120)]

    posted_urls = load_posted_urls()
    posted_this_time = 0

    print(f"ğŸ“° æœ€æ–°è¨˜äº‹å–å¾—: {len(articles)}ä»¶ / æŠ•ç¨¿æ¸ˆã¿: {len(posted_urls)}ä»¶")

    for article in articles:
        print(f"ğŸ” è¨˜äº‹ç¢ºèª: {article['title']} / æŠ•ç¨¿æ—¥: {article['published']}")

        if article["link"] in posted_urls:
            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {article['title']}")
            continue

        summary = summarize_article(article["title"], article["summary"], article["link"])
        if not summary:
            print(f"âš ï¸ è¦ç´„å¤±æ•—: {article['title']}")
            continue

        post_to_discord(summary)
        save_posted_url(article["link"])
        posted_this_time += 1

        print(f"âœ… æŠ•ç¨¿å®Œäº†: {article['title']}")

    print(f"ğŸ“¦ æŠ•ç¨¿ä»¶æ•°ï¼ˆä»Šå›ï¼‰: {posted_this_time}ä»¶")

if __name__ == "__main__":
    run()
